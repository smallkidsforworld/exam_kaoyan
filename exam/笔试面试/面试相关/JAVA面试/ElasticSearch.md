# [ElasticSearch](https://www.cnblogs.com/-wenli/p/12763887.html)
   - 节点分类
      - Master节点:主要是负责维护集群的状态,像所有节点的信息,所有的索引和相关的Mapping关系,配置信息,分片的路由等;
      - DataNode节点：数据节点,这个节点主要负责数据的存储,在数据扩展上起到了至关重要的作用。也就是说读写数据都会找到相应的Data Node节点.
      - CoordinatingNode:负责接收Client 的请求,将请求分发到合适的节点,最终把结果汇集到一起
   - **Master选举:**
      - 对所有可以成为master的节点,根据nodeId字典排序，每次选举每个节点都把自己所知道节点排一次序，然后选出第一个节点，暂且认为它是master节点。
      - 如果对某个节点的投票数达到一定的值（可以成为master节点数n/2+1）并且该节点自己也选举自己，那这个节点就是master。否则重新选举一直到满足上述条件。
      - <font color="yellow">脑裂问题:脑裂是指在主备切换时，由于切换不彻底或其他原因，导致客户端和Slave误以为出现两个active master，最终使得整个集群处于混乱状态</font>
         - 当集群master候选数量不小于3个时，可以通过设置最少投票通过数量超过所有候选节点一半以上来解决脑裂问题；
         - 当候选数量为两个时，只能修改为唯一的一个master候选，其他作为data节点，避免脑裂问题。
   - 分片:当有大量的文档时，由于硬件限制,为提高数据响应速度,数据可以分为较小的分片。每个分片放到不同的服务器上
      - 创建索引时候指明分片个数.
      - 写入分片设定:找到写入的分片.
      ```
      shard = hash(routing) % number_of_primary_shards
      ```
      - 主分片:来解决数据水平扩展的问题。通过主分片,可以将数据分布到集群内的所有节点之上
      - 复制分片只是主分片的一个副本，它可以防止硬件故障导致的数据丢失，同时可以提供读请求，比如搜索或者从别的shard取回文档.
      - 分片设定
         - 设置过少
            - a: 导致后续无法增加节点实现水平扩展
            - b: 单个分片的数量太大,导致数据重新分配耗时
         - 分配过多
            - a:影响搜索结果的相关性打分,影响统计结果的准确性
            - b:单个节点上过多的分片,会导致资源浪费,同时也会影响性能

      